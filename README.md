# DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration (ACM MM 2025)

Official Pytorch Implementation of **"DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration"** by Weiran Chen, Guiqian Zhu, Ying Li, Yi Ji and Chunping Liu.

## Abstract

Few-shot font generation aims to create new fonts with a limited number of glyph references. It can be used to significantly reduce the labor cost of manual font design. However, due to the variety and complexity of font styles, the results generated by existing methods often suffer from visible defects, such as stroke errors, artifacts and blurriness. To address these issues, we propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid Module (DAHM). Specifically, we introduce two synergistic attention blocks: the component attention block that leverages component information from content images to guide the style transfer process, and the relation attention block that further refines spatial relationships through interacting the content feature with both original and stylized component-wise representations. These two blocks collaborate to preserve accurate character shapes and stylistic textures. Moreover, we also design a corner consistency loss and an elastic mesh feature loss to better improve geometric alignment. Extensive experiments show that our DA-Font outperforms the state-of-the-art methods across diverse font styles and characters, demonstrating its effectiveness in enhancing structural integrity and local fidelity. 

<center>
  <img 
    src="/Paper_IMG/Task_definition.jpg" 
    style="width: 60%; height: auto;"
  >
</center>

## Dependencies
>python >= 3.8
>torch >= 1.7.1
>torchvision >= 0.8.2
>sconf >= 0.2.5
>lmdb >= 1.6.2
>opencv-python >= 4.10.0.84
>scipy >= 1.10.1

## Data Preparation
### Images and Characters
1)  Collect a series of '.ttf'(TrueType) or '.otf'(OpenType) files to generate images for training models. and divide them into source font and training set and test set. In order to better learn different styles, there should be differences and diversity in font styles in the training set. The fonts we used in our paper can be found in [here](https://www.foundertype.com/).  

2)  Secondly, specify the characters to be generated (including training characters and test characters), eg the first-level Chinese character table contains 3500 Chinese characters. 

* >{乙、十、丁、厂、七、卜、人、入、儿、匕、几、九、力、刀、乃、又、干、三、七、干、...、etc}

3)  After that, draw all font images via ```./datasets/font2image.py```.
* Organize directories structure as below: 
  > Font Directory  
  > |--| content  
  > |&#8195; --| content_font  
  > |&#8195; &#8195; --| content_font_char1.png  
  > |&#8195; &#8195; --| content_font_char2.png  
  > |&#8195; &#8195; --| ...  
  > |--| train  
  > |&#8195; --| train_font1  
  > |&#8195; --| train_font2  
  > |&#8195; &#8195; --| train_font2_char1.png  
  > |&#8195; &#8195; --| train_font2_char2.png  
  > |&#8195; &#8195; --| ...  
  > |&#8195; --| ...  
  > |--| val  
  > |&#8195; --| val_font1  
  > |&#8195; --| val_font2  
  > |&#8195; &#8195; --| val_font2_char1.png  
  > |&#8195; &#8195; --| val_font2_char2.png  
  > |&#8195; &#8195; --| ...  
  > |&#8195; --| ...

### Build meta files and lmdb environment
1. Split all characters into train characters and val characters with unicode format and save them into json files, you can convert the utf8 format to unicode by using ```hex(ord(ch))[2:].upper():```, examples can be found in ```./meta/```. 
* > train_unis: ["4E00", "4E01", ...]  
  > val_unis: ["9576", "501F", ...]

2. Run script ```./build_trainset.sh```
* ```
  python3 ./build_dataset/build_meta4train.py \
  --saving_dir ./results/your_task_name/ \
  --content_font path\to\content \
  --train_font_dir path\to\training_font \
  --val_font_dir path\to\validation_font \
  --seen_unis_file path\to\train_unis.json \
  --unseen_unis_file path\to\val_unis.json 
  ```




## Acknowledgements
Our project is based on [VQ-Font](https://github.com/awei669/VQ-Font) and [FsFont](https://github.com/tlc121/FsFont). We would like to express our sincere gratitude to our collaborators for their valuable supports and to the reviewers for their insightful feedback and suggestions.
