# DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration (ACM MM 2025)

Official Pytorch Implementation of **"DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration"** by Weiran Chen, Guiqian Zhu, Ying Li, Yi Ji and Chunping Liu.

## Abstract

Few-shot font generation aims to create new fonts with a limited number of glyph references. It can be used to significantly reduce the labor cost of manual font design. However, due to the variety and complexity of font styles, the results generated by existing methods often suffer from visible defects, such as stroke errors, artifacts and blurriness. To address these issues, we propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid Module (DAHM). Specifically, we introduce two synergistic attention blocks: the component attention block that leverages component information from content images to guide the style transfer process, and the relation attention block that further refines spatial relationships through interacting the content feature with both original and stylized component-wise representations. These two blocks collaborate to preserve accurate character shapes and stylistic textures. Moreover, we also design a corner consistency loss and an elastic mesh feature loss to better improve geometric alignment. Extensive experiments show that our DA-Font outperforms the state-of-the-art methods across diverse font styles and characters, demonstrating its effectiveness in enhancing structural integrity and local fidelity. 

<div align="center">
  <img src="/Paper_IMG/Task_definition.jpg" width="70%">
</div>

## Dependencies

> python >= 3.8  
> torch >= 1.7.1  
> torchvision >= 0.8.2  
> sconf >= 0.2.5  
> lmdb >= 1.6.2  
> opencv-python >= 4.10.0.84  
> scipy >= 1.10.1

## Data Preparation
### Images and Characters
1)  First, collect a set of font files in .ttf (TrueType Font) or .otf (OpenType Font) format. These fonts should be divided into three groups: content font, training fonts, and validation fonts. To ensure the model effectively learns style variations, it's recommended that the training fonts exhibit noticeable style diversity. In our experiments, the fonts were sourced from [here](https://www.foundertype.com/).  

2)  Next, determine the target character set for both training and testing. For example, the first-level Chinese character set contains 3500 commonly used characters:

 >{梦、飞、龙、书、雪、月、花、山、海、云、星、雨、竹、茶、琴、棋、画、诗、酒、剑、...、etc}

3)  Once the fonts and characters are prepared, use the script ```./datasets/font2image.py``` to render character images from the fonts. The directory structure should be organized as follows:
```
Font Directory
|-- content
|   |-- content_font
|   |   |-- content_font_char1.png
|   |   |-- content_font_char2.png
|   |   |-- ...
|-- train
|   |-- train_font1
|   |-- train_font2
|   |-- train_font3
|   |   |-- train_font3_char1.png
|   |   |-- train_font3_char2.png
|   |   |-- ...
|   |-- ...
|-- val
|   |-- val_font1
|   |-- val_font2
|   |-- val_font3
|   |   |-- val_font3_char1.png
|   |   |-- val_font3_char2.png
|   |   |-- ...
|   |-- ...
```

### Construct meta files and LMDB environment
1. The characters need to be split into training and validation sets, with each character saved in Unicode format. You can convert characters to Unicode using ```hex(ord(ch))[2:].upper():```, examples can be found in ```./meta/```.

2. Execute the following command to generate the LMDB files:
```
  python3 ./build_dataset/build_meta4train.py \
  --saving_dir ./results/your_task_name/ \
  --content_font path\to\content \
  --train_font_dir path\to\training_font \
  --val_font_dir path\to\validation_font \
  --seen_unis_file path\to\train_unis.json \
  --unseen_unis_file path\to\val_unis.json 
  ```

## Training Process
The training process is divided into two stages: 1）Pre-training the content encoder and codebook via [VQ-VAE](https://arxiv.org/abs/1711.00937), 2）Training the few shot font generation model via [GAN](https://dl.acm.org/doi/abs/10.1145/3422622). 
### Pre-train VQ-VAE
When pre-training VQ-VAE, the reconstructed character object comes from train_unis in the content font, The training process can be found at ```./model/VQ-VAE.ipynb```. 

Then use the pre-trained content encoder to calculate a similarity between all training and test characters and store it as a dictionary.
> {'4E07': {'4E01': 0.2143, '4E03': 0.2374, ...}, '4E08': {'4E01': 0.1137, '4E03': 0.1020, ...}, ...}


### Train DA-Font

Modify the configuration in the file ```./cfgs/custom.yaml```

Some Keys
* work_dir: the root directory for saved results. (keep same with the `saving_dir` above) 
* data_path: path to data lmdb environment. (`saving_dir/lmdb`)
* data_meta: path to train meta file. (`saving_dir/meta`)
* content_font: the name of font you want to use as source font.
* all_content_char_json: the json file which stores all train and val characters.  
* other values are hyperparameters for training.

 Run scripts
 ```
  python3 train.py task_name cfgs/custom.yaml
    #--resume \path\to\your\pretrain_model.pdparams
  ```

## Infer DA-Font

```
  python3 inference.py ./cfgs/custom.yaml \
  --weight \path\to\saved_model.pdparams \
  --content_font \path\to\content_imgs \
  --img_path \path\to\test_imgs \
  --saving_root ./infer_res
  ```

## Acknowledgements
Our project is based on [VQ-Font](https://github.com/awei669/VQ-Font) and [FsFont](https://github.com/tlc121/FsFont). We would like to express our sincere gratitude to our collaborators for their valuable supports and to the reviewers for their insightful feedback and suggestions.

## Citation

```
@InProceedings{Chen_2025_MM,
    author    = {Weiran Chen and Guiqian Zhu and Ying Li and Yi Ji and Chunping Liu},
    title     = {DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration},
    booktitle = {Proceedings of the {ACM} International Conference on Multimedia, {MM}},
    month     = {October},
    year      = {2025},
    pages     = {xxxx-xxxX}
}
```
